# Model Configuration
model_name: deepseek-ai/DeepSeek-R1-Distill-Qwen-14B

# Dataset Configuration
dataset_name: joyfine/LLM_Bias_Detection_CoT_Training

# Training Parameters
output_dir: /qumulo/haoyan/DeepSeek-R1-Distill-Qwen-14B-full-fine-tuned_CoT_with_bias_type
per_device_train_batch_size: 1
per_device_eval_batch_size: 4
gradient_accumulation_steps: 1
epochs: 1
learning_rate: 1.0e-5
mixed_precision: "bf16" 
logging_strategy: "steps"
evaluation_strategy: "steps"
save_strategy: "steps"
logging_steps: 20
eval_steps: 20
save_steps: 80
dataset_text_field: "text"
max_seq_length: 3584
dataset_num_proc: 4
seed: 3407
save_only_model: True
# load_best_model_at_end: False
# metric_for_best_model: "eval_loss"
# greater_is_better: False
save_total_limit: 5

# Optimizer & Scheduler
optim: "adamw_torch"
weight_decay: 0.01
lr_scheduler_type: "cosine"
warmup_ratio: 0.1

# Weights & Biases (W&B) Logging
wandb_project: "Bias-Detection-full-Training"
wandb_run_name: "DeepSeek-R1-Distill-Qwen-14B-full-fine-tuned_CoT_with_bias_type"
report_to: "wandb" 
