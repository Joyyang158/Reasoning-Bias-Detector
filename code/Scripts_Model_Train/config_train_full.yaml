# Model Configuration
# model_name: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
# model_name: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
model_name: Qwen/Qwen2.5-Math-1.5B

# Dataset Configuration
dataset_name: joyfine/LLM_Bias_Detection_CoT_Training

# Training Parameters
# output_dir: /qumulo/haoyan/DeepSeek-R1-Distill-Llama-8B-full-fine-tuned_CoT_with_bias_type
output_dir: /qumulo/haoyan/Qwen2.5-Math-1.5B-full-fine-tuned_QA_Instruction_with_bias_type
per_device_train_batch_size: 1
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4
epochs: 5
learning_rate: 1.0e-5
mixed_precision: "bf16" 
logging_strategy: "steps"
evaluation_strategy: "steps"
save_strategy: "steps"
logging_steps: 20
eval_steps: 20
save_steps: 20
dataset_text_field: "text"
max_seq_length: 4096
dataset_num_proc: 4
seed: 3407
save_only_model: True
load_best_model_at_end: True
metric_for_best_model: "eval_loss"
greater_is_better: False
save_total_limit: 1

# Optimizer & Scheduler
optim: "adamw_torch"
weight_decay: 0.01
lr_scheduler_type: "cosine"
warmup_ratio: 0.1

# Weights & Biases (W&B) Logging
wandb_project: "Bias-Detection-full-Training"
wandb_run_name: "Qwen2.5-Math-1.5B-full-fine-tuned_QA_Instruction_with_bias_type"
report_to: "wandb" 
