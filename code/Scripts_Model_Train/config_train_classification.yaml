# Model Configuration
# model_name: "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"

# Dataset Configuration
dataset_name: "joyfine/LLM_Bias_Detection_CoT_Training"

# Training Parameters
# output_dir: "/qumulo/haoyan/deepseek-8b-full-fine-tuned"
output_dir: "/qumulo/haoyan/DeepSeek-R1-Distill-Qwen-1.5B-full-fine-tuned_QA_Classification_with_bias_type"
per_device_train_batch_size: 1
per_device_eval_batch_size: 4
gradient_accumulation_steps: 4
epochs: 5
learning_rate: 1.0e-5
mixed_precision: "bf16" 
logging_strategy: "steps"
evaluation_strategy: "steps"
save_strategy: "steps"
logging_steps: 20
eval_steps: 20
save_steps: 20
max_seq_length: 4096
dataset_num_proc: 4
seed: 3407
save_only_model: True
load_best_model_at_end: True
metric_for_best_model: "accuracy"
greater_is_better: True
save_total_limit: 1

# Optimizer & Scheduler
optim: "adamw_torch"
weight_decay: 0.01
lr_scheduler_type: "cosine"
warmup_ratio: 0.1

# Weights & Biases (W&B) Logging
wandb_project: "Bias-Detection-full-Training"
wandb_run_name: "DeepSeek-R1-Distill-Qwen-1.5B-full-fine-tuned_QA_Classification_with_bias_type"
report_to: "wandb" 
